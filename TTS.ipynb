{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPEETdhyz1czNICb75RzH/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a1d88a520984282840d9cd5ad54a9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f53441d17114364a92342a2626bc7ae",
              "IPY_MODEL_d760068a93af4cad943252554b91ab0f",
              "IPY_MODEL_d7ed61bff0284ece919de1b68130509e"
            ],
            "layout": "IPY_MODEL_3243d73bcf1c46899fb8873a03e25e49"
          }
        },
        "5f53441d17114364a92342a2626bc7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22cee060a8994847aa54902448f04b63",
            "placeholder": "​",
            "style": "IPY_MODEL_b7d9177c16a342c78867145b98f6282f",
            "value": "config.json: 100%"
          }
        },
        "d760068a93af4cad943252554b91ab0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943995c5ead5440b8d61cc5d52beb4d2",
            "max": 1403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f6987d7ae164814aaaa2f3567cba2b1",
            "value": 1403
          }
        },
        "d7ed61bff0284ece919de1b68130509e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc1c8924a614cf18e6fb3562c3a8678",
            "placeholder": "​",
            "style": "IPY_MODEL_1bece5c4333a484ea8386f18c102e174",
            "value": " 1.40k/1.40k [00:00&lt;00:00, 28.8kB/s]"
          }
        },
        "3243d73bcf1c46899fb8873a03e25e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cee060a8994847aa54902448f04b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d9177c16a342c78867145b98f6282f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "943995c5ead5440b8d61cc5d52beb4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6987d7ae164814aaaa2f3567cba2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdc1c8924a614cf18e6fb3562c3a8678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bece5c4333a484ea8386f18c102e174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3024908fb3741199b9861462cd056fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37d7e4e1771f4008a2299b6dae8e7db7",
              "IPY_MODEL_e1b60cc0fc514016885cb9e694201567",
              "IPY_MODEL_4c32f7499cfb4b0e8cbeaf85c145c52a"
            ],
            "layout": "IPY_MODEL_55e694c6b9fa4a8487a1e9d0ac4f37ab"
          }
        },
        "37d7e4e1771f4008a2299b6dae8e7db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abdb36c198334fb0a48f0aa4ae768509",
            "placeholder": "​",
            "style": "IPY_MODEL_c1cc090e652b4ef09a28bddd33a51672",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "e1b60cc0fc514016885cb9e694201567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e50b5449ed147c899ec46ce3fcb5ec4",
            "max": 312087009,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1024d33a46243ff9fd759dbb9083455",
            "value": 312087009
          }
        },
        "4c32f7499cfb4b0e8cbeaf85c145c52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316740d1905a4f64b5d7c32fc54e27a8",
            "placeholder": "​",
            "style": "IPY_MODEL_977c201d434942b48426fc37f7665c8a",
            "value": " 312M/312M [00:03&lt;00:00, 129MB/s]"
          }
        },
        "55e694c6b9fa4a8487a1e9d0ac4f37ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdb36c198334fb0a48f0aa4ae768509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1cc090e652b4ef09a28bddd33a51672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e50b5449ed147c899ec46ce3fcb5ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1024d33a46243ff9fd759dbb9083455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "316740d1905a4f64b5d7c32fc54e27a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977c201d434942b48426fc37f7665c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "619b7988e6f94f6aaee6d4d4ec46e419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb84e2f9a4a149b29583e12959c0d552",
              "IPY_MODEL_e421d7d69ec948efa671f33d040b5cbf",
              "IPY_MODEL_964e2beafa8d4cbc996d81c0bdcd0548"
            ],
            "layout": "IPY_MODEL_f66cc1acb384455c9eeda86ba662d945"
          }
        },
        "eb84e2f9a4a149b29583e12959c0d552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_276ac35ae33e48f48b3f676e5893525a",
            "placeholder": "​",
            "style": "IPY_MODEL_34ee97b84ef240a982fc1c44f04e6c92",
            "value": "generation_config.json: 100%"
          }
        },
        "e421d7d69ec948efa671f33d040b5cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7641dfd3cc76475f9c751fa2cc06bdae",
            "max": 293,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c1f350274b24156a9d78e61762f3919",
            "value": 293
          }
        },
        "964e2beafa8d4cbc996d81c0bdcd0548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718322aea83b4f889c92b6cc81329be4",
            "placeholder": "​",
            "style": "IPY_MODEL_6ffc6f9b17ca4143a7aa6a672a315177",
            "value": " 293/293 [00:00&lt;00:00, 21.5kB/s]"
          }
        },
        "f66cc1acb384455c9eeda86ba662d945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276ac35ae33e48f48b3f676e5893525a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ee97b84ef240a982fc1c44f04e6c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7641dfd3cc76475f9c751fa2cc06bdae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c1f350274b24156a9d78e61762f3919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "718322aea83b4f889c92b6cc81329be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffc6f9b17ca4143a7aa6a672a315177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8def9dd093c74719a820b59dbaf3084a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_922cf4ae6ea343e584418c2e4960ec0c",
              "IPY_MODEL_2988d91762154e5a84390be344cd51bc",
              "IPY_MODEL_aaf2ba9341b44cc3bf8df84890d121a4"
            ],
            "layout": "IPY_MODEL_881a811584964935a1a3c39343b7a64a"
          }
        },
        "922cf4ae6ea343e584418c2e4960ec0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27fa0489ac6047889c4ecb6a79812bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_bbc1475c97234d4d939170b94d0f5d6d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2988d91762154e5a84390be344cd51bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfbb00fc2d4f409fb3a428dee3cd17e9",
            "max": 44,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6fdbdc40bea43328ad7d85b868a1e28",
            "value": 44
          }
        },
        "aaf2ba9341b44cc3bf8df84890d121a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442c92a6329944ec905b6a786cfcb65d",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4308264f1645a18cf1d3cc13318b0c",
            "value": " 44.0/44.0 [00:00&lt;00:00, 3.05kB/s]"
          }
        },
        "881a811584964935a1a3c39343b7a64a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27fa0489ac6047889c4ecb6a79812bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc1475c97234d4d939170b94d0f5d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfbb00fc2d4f409fb3a428dee3cd17e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6fdbdc40bea43328ad7d85b868a1e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "442c92a6329944ec905b6a786cfcb65d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4308264f1645a18cf1d3cc13318b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed09f6a44494bf9ab640476ddeba455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e63409b96be46e4878426075a1e3303",
              "IPY_MODEL_f8b2db2a4b134069a88e0333fc870214",
              "IPY_MODEL_ac2d35c4d3794d3faf0a6a53a2e533e5"
            ],
            "layout": "IPY_MODEL_3ad28a0330fb49639c10be1e93fd737e"
          }
        },
        "5e63409b96be46e4878426075a1e3303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d96ab26cb794cc8bbb502499ae07a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_cb386ed3259648e6ba3408c16371d464",
            "value": "source.spm: 100%"
          }
        },
        "f8b2db2a4b134069a88e0333fc870214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab160c335a1a4c41950e948fab4676c4",
            "max": 806435,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a672abcfbeb4374b0d15387740ec1e5",
            "value": 806435
          }
        },
        "ac2d35c4d3794d3faf0a6a53a2e533e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2193b39b8fc746ce9859d0f76ed4583e",
            "placeholder": "​",
            "style": "IPY_MODEL_f3c5847375174aea868ca7c583c3a127",
            "value": " 806k/806k [00:00&lt;00:00, 6.06MB/s]"
          }
        },
        "3ad28a0330fb49639c10be1e93fd737e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d96ab26cb794cc8bbb502499ae07a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb386ed3259648e6ba3408c16371d464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab160c335a1a4c41950e948fab4676c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a672abcfbeb4374b0d15387740ec1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2193b39b8fc746ce9859d0f76ed4583e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c5847375174aea868ca7c583c3a127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "565afd87d245499fb13d85c24aa8d0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f50f35be922e46078667e9f953cc1872",
              "IPY_MODEL_69919f5070db4a91a149cebe03d7615b",
              "IPY_MODEL_ee98e0f2966348f9a203efa4beb83b3f"
            ],
            "layout": "IPY_MODEL_65cf19c451e74a238ca751f57479ce1e"
          }
        },
        "f50f35be922e46078667e9f953cc1872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0082f215390d4a03820ffd860ca589ac",
            "placeholder": "​",
            "style": "IPY_MODEL_2683aaee6b0d4530bbacfd72de6257ad",
            "value": "target.spm: 100%"
          }
        },
        "69919f5070db4a91a149cebe03d7615b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801a75ab157b48ac98dc432005ef6747",
            "max": 804600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54aa4da93f2047fca8c4efe50f0772dc",
            "value": 804600
          }
        },
        "ee98e0f2966348f9a203efa4beb83b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ef0f08cc5d4897bb9535b288ceb2a3",
            "placeholder": "​",
            "style": "IPY_MODEL_d4cbaafa985548d3b5b6e35e8c966557",
            "value": " 805k/805k [00:00&lt;00:00, 4.14MB/s]"
          }
        },
        "65cf19c451e74a238ca751f57479ce1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0082f215390d4a03820ffd860ca589ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2683aaee6b0d4530bbacfd72de6257ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "801a75ab157b48ac98dc432005ef6747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54aa4da93f2047fca8c4efe50f0772dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03ef0f08cc5d4897bb9535b288ceb2a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4cbaafa985548d3b5b6e35e8c966557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59050071f744bbb9da3abc35b18883d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74f395f0004f4434b9d077c9344ae491",
              "IPY_MODEL_0c0e84424df441f1a9945cc155f393f1",
              "IPY_MODEL_08220cf957784d4cb91a123a2c06b199"
            ],
            "layout": "IPY_MODEL_4a23026f58d2419ca8b64558ef528e73"
          }
        },
        "74f395f0004f4434b9d077c9344ae491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_776d592b26744d838879860be29c970b",
            "placeholder": "​",
            "style": "IPY_MODEL_e8697d8c87dd4011b0a4cf3863e08846",
            "value": "vocab.json: 100%"
          }
        },
        "0c0e84424df441f1a9945cc155f393f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d1d963cf9e425db7c0241483f9c9d7",
            "max": 1617791,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_685167286add44b2a8b9c3185d75fc7f",
            "value": 1617791
          }
        },
        "08220cf957784d4cb91a123a2c06b199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f884c595489945a79033d3d6adb94a2b",
            "placeholder": "​",
            "style": "IPY_MODEL_19a7f478e2b24515a94a6d7011873e16",
            "value": " 1.62M/1.62M [00:00&lt;00:00, 8.07MB/s]"
          }
        },
        "4a23026f58d2419ca8b64558ef528e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776d592b26744d838879860be29c970b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8697d8c87dd4011b0a4cf3863e08846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56d1d963cf9e425db7c0241483f9c9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685167286add44b2a8b9c3185d75fc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f884c595489945a79033d3d6adb94a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a7f478e2b24515a94a6d7011873e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burke-up/test_celery/blob/main/TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wz8SWLuE7yNs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers[sentencepiece] accelerate"
      ],
      "metadata": {
        "id": "U_Xj1cln_RgN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "设置保存路径"
      ],
      "metadata": {
        "id": "Zs__YR7SvhkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_save_path = \"path_to_save_video\"\n",
        "audio_save_path = \"path_to_save_audio\""
      ],
      "metadata": {
        "id": "NO71pdpv82Jc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下载youtube视频，并提取音频，保存到文件中"
      ],
      "metadata": {
        "id": "5VOw50FHvWGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "video_id = \"eqOfr4AGLk8\"\n",
        "\n",
        "# 输入YouTube视频的URL\n",
        "video_url = \"https://www.youtube.com/watch?v=%s\"%(video_id)\n",
        "\n",
        "# 创建YouTube对象\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "# 获取视频的所有流\n",
        "streams = yt.streams\n",
        "\n",
        "# 选择要下载的视频流，这里选择第一个mp4视频流\n",
        "video_stream = streams.filter(file_extension='mp4').first()\n",
        "\n",
        "# 下载视频\n",
        "video_stream.download(video_save_path)\n",
        "\n",
        "# 提取音频，选择一个音频流\n",
        "audio_stream = streams.filter(only_audio=True).first()\n",
        "\n",
        "# 下载音频\n",
        "audio_stream.download(audio_save_path)\n",
        "\n",
        "print(\"下载完成！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0VhTevs72hE",
        "outputId": "861238c9-7034-4234-eba6-bfc89caaf6f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下载完成！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看保存的音频文件"
      ],
      "metadata": {
        "id": "a6KOJ-nzvpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls path_to_save_audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8jauiqc8HtU",
        "outputId": "2ecd6461-2f1d-4a71-efdc-297ad2327c10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'LangChain Data Loaders Tokenizers Chunking and Datasets - Data Prep 101.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以更友好的方式显示音频文件"
      ],
      "metadata": {
        "id": "AnCzDgsfvuG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "files = os.listdir(audio_save_path)\n",
        "audio_file = \"\"\n",
        "if files:\n",
        "  audio_file = os.path.join(audio_save_path, files[0])\n",
        "  display(Audio(audio_file))\n",
        "else:\n",
        "  print(\"not found audio\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p28lQqFa8Q3J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用whisper-large-v3，将音频转换为文本，并提取时间戳"
      ],
      "metadata": {
        "id": "LsRB5tJMv5Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=30,\n",
        "    batch_size=16,\n",
        "    return_timestamps=True,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "sample = audio_file\n",
        "\n"
      ],
      "metadata": {
        "id": "SQe3x4dVEF1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(sample)\n",
        "import json\n",
        "print(json.dumps(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ycyfpNIFzd",
        "outputId": "b7903e0f-5fda-41f0-e5cd-0ed46961a67c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\": \" In this video, we are going to take a look at what we need to do and what we need to consider when we are chunking text for large language models. The best way I can think of demonstrating this is to walk through an example. Now, we're going to really go with what I believe is kind of like a rule of thumb that I tend to use when I'm chunking text in order to put into a large language model. And it doesn't necessarily apply to every use case. Every use case is slightly different. But I think this is a pretty good approach, at least when we're using retrieval augmentation and large language models, which I think is where the chunking question kind of comes up most often. So let's jump straight into it. In this example what we're going to be doing is taking the langchain docs here literally every page on this website and we're going to be downloading those taking each one of these pages and I'm going to be splitting them into more reasonably sized chunks. Now how are we going to do this? We're going to take a look at this notebook here. Now, if you'd like to follow along with the code, you can also run this notebook. I will leave a link to it, which will appear somewhere near the top of the video right now. Now, to get started, we're going to be using a few Python libraries. Langchain is a pretty big one here. So not only is it the documentation that we're downloading, but it's also going to be how we download that documentation, and it's also going to be how we split that documentation into chunks. And another dependency here is the tick token, tokenizer, we'll talk about that later. And we're just gonna visualize and make things a little bit easier to follow with these libraries here. So in this example first thing I'm going to do is download all of the docs from LineChain. So everything is contained within this is the top level page of the LineChain docs. We're going to save everything into this directory here and we are going to say we want to get all of the dot HTML files. Okay so we run that and that will take a moment just download everything that there's a lot in there. My internet connection is also pretty slow so it will probably take me a moment. But let's go ahead and just have a look at where these are being downloaded. So if we come over to the left here, we can see there is the RTDocs repository there. And inside the RTDocs, we have this line chain read docs, enlatus, which is just kind of like the path of our docs. And okay, cool. So in there, you can see everything's been downloaded. We have like the index page, which I think is the top level page. And you can see it's just HTML. Okay. So it's kind of like, we're not going to process this. We're going to use long chain to clean this up. But if we come down a little bit I think maybe we can see something okay so this is like the the first page welcome to Langchain LLMs are emerging as a transformative technology so on and so on okay and we have some other things of the pages yeah we're just going to process all of this. So back to our code, it's done downloading now. We can come down to here and what we're going to do is use the LankChain document loaders and we're going to use a ReadDops loader. So ReadDops is a specific template that is used quite often for documentation for code libraries. And LimeChain includes a document loader that is specifically built for reading that type of documentation or those HTML pages and processing them into a nicer format. So it's really easy to use it. We just point it to our directory that we just created. And what are we doing here so we're loading those docs and here I'm just printing out the length of those docs so that we can see okay we have 390 HTML pages that have been downloaded there some reason okay so when I when I ran this about an hour ago they they actually had 389 now they have 390 pages so it's already out of date cool all right let's have a look at one of those pages so we have this we have this document object inside that we have page content which is all about text all right if we want to print that in a nicer format, we can see this. Looks pretty good. There is some kind of messy parts of this, but it's not really a problem. We could try and process that if we wanted to, but honestly, I don't really think it's worth it because the large range model can handle this very easily. So, yeah, I personally wouldn't really bother with that. I just take it as it is. Now, at the end of this object, we come right to the end, if it lets me, we'll see that we have this metadata here. we'll see that we have this metadata here. Inside the metadata, we have the source, which is in this case like the file path. But fortunately, the way that we've set this up is that we can just replace RTDocs with HTTPS, and that will give us the URL for this particular file. Let's come down here and you can see that's what I'm doing here, replace RTDocs with HTTPS. Cool. And then we can click that and we come over to here. Now, this is where we start talking about the chunking of what we're doing. When we are thinking about chunking, there are a few things to consider. So the first thing to consider is how much text or how many tokens can our large language model or whatever process is we're doing, how many tokens can it handle? What is optimal for our particular use case? is optimal for our particular use case. The use case that I'm envisioning here is retrieval augmentation for question answering using a larger language model. So what does that mean exactly? It's probably best if I draw it out. So we're going to have our large language model over here and we're going to ask it a question. So we have a question over here supposed to be a queue it's fine so we have our question like we're gonna say what is the LM chain in Langchain right if we pass that straight into our large language model at the moment using a GPT 3.5 turbo even GPT 4 they can't answer that question because they don't know what the langchain library is. So in this scenario, what we would do is we'd go to a vector database. You know, we don't really need to go into too much detail here. We go to a vector database, which is where we saw all of the documents that we're processing now. So all those langchain docs, they would end up within that space and they would be retrieved and we would pass in like five or so of these chunks of text that are relevant to our particular query alongside our original query. So what you'd end up with is rather than, let's say this is your prompt, you typically have your query, rather than just a query, you'd have your query and then you'd also have these five bits of relevant information below the query. And that would all go into the large language model. And you would essentially say to it you'd probably have some instructions near the top and those instructions would say I want you to answer this question you'd maybe give the the questionnaire to give it a bit later on using the context that we have provided and you would basically in front of these contexts you would write like context, okay? And the large language model will answer the question based on those contexts, right? So that's the scenario we're envisioning here. And in this scenario, if we want to input five of these contexts into each one of our retrieval augmented queries, we need to think okay what is the max token limit of our large language model and how much of that space can be reserved for these contexts so in this scenario let's say that we're using gpt 3.5 turbo the token limit for gpt 3.5 Turbo. The token limit for GPT 3.5 Turbo is something like 4096. So this includes both, all right, so you have your large language model. I'm going to put that here. This is, pretend this is your large language model. This 4096 includes the input to the large language model, so all your input tokens, and also all of your generated output tokens. Okay. And so basically, we can't just use that full 4000 tokens on the input, We need to leave some space for the output. And also within the input, we have other components, right? So it's not just the context, but we also have the query. I mean, that's supposed to say query. And as well as that, we might also have some instructions. Don't know why am I writing so bad. And as well as the instructions, we might also have a bit of tracked history if this is a chatbot. Okay. So basically, the amount of context that we can feed in is actually pretty limited. In this scenario, let's just assume that we can we can pass in a context of around half of the 4,000 tokens so we'll say 2,000 is going to be our limit. Okay 2,000 is our limit. That means we need to divide that by 5 because those 2,000 tokens need to be shared by our five contacts which leaves us with about 400 of these tokens per context okay so that's our maximum chunk size now one question that we might have here is could we reduce the number of tokens further and for sure we can okay so i would say the minimum number of tokens that you need within a context is for you to read this context does it make sense right if you have enough words in there for that context to make sense to you as a as a human being then that means that it is probably enough to feed as a chunk of text into a large language model into a embedding model and so on so if that chunk of text has enough text in there to have some sort of meaning to itself then the chunk is probably big enough so as long as you satisfy that that should be the criteria for your minimum size of that chunk of text. Naturally, for the maximum size of a chunk of text, we have the 400 tokens that we just calculated now. So with all that in mind, we need to take a look at how we would actually calculate the size of these chunks. Because we're not basing this on character length, we're basing this on token length. So in order to do that we need to look at how to tokenize text using the same tokenizer that our large language model uses and then we can actually count the number of tokens within each chunk so getting started with that we are going to be using the tick token tokenizer now this is specific to openai models obviously if you're using cohere hugging face and so on this is going to be a slightly different approach so first we want to get our encoding. So there are multiple TikTok and tokenizers that open our users. This is just one of those. Now let's initialize that. And I will talk about a little bit about where we're getting these encoders from. So you can actually find details for the tokenizer at this link here. So this link is in the GitHub repo, TikTok tick token tick token model.py okay so i'm going to click through to that okay so this is in the openai tick token repository on github and you can see we have this model to encoding dictionary here and within this you can see that we have a mapping from each of the models to the particular tokenizer that it uses. We are going to use the GPT 3.5 Turbo model, which uses the CL 100k base. And I would say, I think most of the more recent models, like the models that you'd be using at the time of recording this video, they all use this encoder. Okay, so the embeddings model that is the most up-to-date uses cl 100k base the you know chat gpt's gpt 3.5 turbo uses cl 100k base gpt4 also uses it the only one that is still kind of a relevant model is the text of energy 003 model. And that is the only relevant model that doesn't use that encoder. So this one uses P50K base. All right, so in reality, you don't even need to go there to find out the encoding that you need to use. You can actually just see this. So take token encoding for model and you can run this, right? So you get the CL 100K base. That's how we know. Now, anything else? I think that is pretty much it. So, okay. So actually here, I'm creating this TickToken length function. So that is gonna take some text. It's going to use the tokenizer to calculate the length of that text in terms of tick token tokens that's important because we we need to use that for our line chain splitter function in a moment so we create that then what we can do is just first before we kind of jump into the whole chunking component, I want to have a look at what the length of documents looks like at the moment. So I'm going to calculate the token counts, the tick token length function. Come to here, we can see the minimum, maximum and average number of tokens. So the smallest document contains just 45 tokens. This is probably i don't know this is probably a page that we don't really need it probably doesn't contain anything useful in that maximum is almost 58 000 tokens which is really big i'm not sure i'm not sure what that is but the average is a bit more normal so 1.3 thousand there so we can kind of visualize the distribution of those of those pages and the amount of tokens they have so the vast majority of pages have a very like they're more towards the 1000 token range as we can sort of see here all right cool now let's continue and we'll start and look at how we're going to chunk everything. So again, we're using line chain here. We're using a text splitter and we're using the recursive character text splitter. Now this is, I think probably one of the best like chunkers or text splitters that line chain offers at the moment. It's very general purpose. They do also offer or text splitters that line chain offers at the moment it's very general purpose they do also offer some text splitters that are more specific to like markdown for example but i you know i i like this one it you can use it for a ton of things so let me just explain it very quickly so basically what it's going to do is it's going to take your length function so the tick token length and it's going to say I need to split your text so that each chunk does not go over this chunk size here so this 400 and it's going to split based on these separators. Okay so the reason we have multiple separators is that it first starts by trying to find double new lines So this is a double new line separator It's going to try and split on that first if it can't find a good split using the double new line Characters it will just try a single new line Then it will try a space and as a very last resort it will just split on anything Okay, okay cool and then one final thing that we have here is this chunk overlap so this chunk overlap is saying for every chunk we are going to overlap it with the next chunk by 20 tokens okay let me let me draw that out so it makes more sense. Okay, so imagine we have a ton of texts, okay? There's loads of texts here. Okay, now we are going to get a chunk of, it's 400 characters, right? So let's say that chunk takes us from here all the way to, let's say here. Okay, so we have 400 characters in this chunk. Then the next chunk if we don't have any chunk overlap would be 400 characters from this so that would be let's say it's to here. Okay but this comes with a problem because we don't know what this information here and this information here is about. So they could be related, right? So we might be missing out on some important information by just splitting in the middle here. So it's important to try and avoid that if possible. And the most naive way or naive approach for doing this is to include a trunk overlap so what we would do is let's say we take the 20 tokens behind this okay so we're gonna go back 20 tokens which maybe comes to here okay so that means that this space here is now going to be shared by the last or the the first chunk and the next chunk which will also bring back the next chunk to something like here right so now we have chunk one here, which goes from here up to here. And then we have chunk two, which is from here to here. Then following on from that, we would also add another chunk overlap for number three. So number three would go from here to let's say here. And finally for number four, we go from like here to here. Okay, so the chunk overlap is just to make sure that we're not missing any important connections between our chunks. Okay, it does mean that we're going to have a little bit more data to solve there. there okay because we're including like these chunks of 20 in multiple places but I think that's usually worth it in terms of the better performance that you can get by not missing out that important information that important connection between chunks okay so we initialize that and then to actually split the text we use the text splitter split text okay we're going to take DOPS 5 and we're going to take the page content okay which is just the plain text. Right so based on the parameters that we set here chunk size of 400 and chunk overlap of 20 using the tick token length token, we get two chunks. Let's have a look at the length of those two chunks. Okay, so the first chunk that we get is 346 tokens. Next one, 247. So both within that max upper end limit of 400. Okay, so you see that it's not going to necessarily split both within that max upper end limit of 400. Okay, so you see that it's not going to necessarily split on the 400 tokens specifically because we have the specific separators that we would like to use, okay? And it's going to optimize preferably for this separator. Okay, so we're not going right up to that limit with every single chunk, which is fine. That's kind of ideal. We don't necessarily need to put in a ton of text there. So that's it for a single document. What we're going to do now is we're going to repeat that over the entire dataset. And the final format that I want to create here is going to look like this okay so we're going to have the id we're going to have our text I'm going to have the source where this text has actually come from okay now one thing that you'll notice here is the id okay so we're going to create an id and that id will be unique to each page okay but we're going to have multiple chunks for each page so that means we're also going to add in this like chunk identifier onto the end of the ID to make sure that every ID for every chunk is actually unique so let me show you how we're going to create that essentially so we have the URL here how we're going to create that. Essentially, so we have the URL here. We're going to replace the RT docs that we have here with the actual HTTPS protocol. And I'm just going to print out so you can see what it is. And then we're going to take that URL. We're going to add it to this hashlib MD5. So this is just a hashing function that is going to take our URL and hash it into kind of like a unique identifier. Right. So this is useful because if we are updating this text at some point in the future or this data set, sorry, we can use the same hashing function to create our unique ids and that means that when we update this particular page it will just overwrite the previous versions of that item all right because we're using the same id but of course we don't we can't use the same id for every single chunk so we also need to add in this here which is like the the chunk identifier right it's just it's just a count of the number of chunks so we can see that being created here so these are just two examples from the previous page that we we just showed so you can see we have the trunk identifier and indeed the chunks are different so this says language model cascades ice primer books socratic models okay whatever let's take a look at what is at the end of the first item and it should be something similar so there should be the overlap that i mentioned right okay so yeah you can see language model cascades, ISE prime books, Socratic models, right same thing, cool. So there is the overlap right now what we need to do is repeat this same logic that we've just created across our entire data set. So to do that same thing that we just did we're going to take the URL out, we're going to create our unique ID, we're going to take chunks using the text splitter, and then we're going to append these all to our documents list here. Okay, that's just going to be where we saw everything. Okay, and now, so the length of the documents an hour ago was a little bit less. Now it 2012 documents so sorry 2,212 documents cool we can now save them to JSON lines file to that we just do this so JSON lines is basically it's what you can see here alright so if we take a look at the documents first five it's this but it's just in can see here, right? So if we take a look at the documents, take a look at the first five, it's this, but it's just in a JSON lines file, okay? So you can see it here, yeah, same thing, right? Okay, and then once you've saved it and you've created your JSONL file, you would just load it from file like this, okay? So you, with open, train JSONL, wherever you saw it, and you just load it iteratively like that. Okay, and you can take a look. Yeah, okay, great. So that's how you would load it. Now, couple of things here. The reason that we're using JSONL and the reason I'm calling this train.jsonl is because this makes it very compatible with HuggingFace datasets which is essentially a way of sharing your dataset with others or just making it more accessible for yourself if you set to being a private dataset. So what I want to do is just show you how we can actually go about doing that as well. So the first thing that we need to do is go to huggingface.co and that will bring you to the first page of Hugging Face which may look different to you because you may not already have an account on Hugging Face. So if you do need an account or you need to sign in there will be a little button over here that says sign up or log in so you would follow that create your account or log in and then you will see something like this at which point you go over to your profile you click new data set we give our data set a name i'm going to call it langchain dots you can obviously call this whatever you want you can set it to private if you want to keep this data set private for me also i'm going to just leave it as public, and you create your dataset. So on here, this is like the page of your dataset, like the homepage of your dataset. You go to Files. You go to Add File, Upload Files. And then you just need to drag in the train.jsonl file to here. So for me, that is here. I'm just gonna go and drag that in. Okay, we go down, commit changes to main. Okay, so we have now uploaded that. We can go click on files here and we'll be able to see that we have the train.jsonl file in there. Now to actually use that in our code, we would need to pip install datasets. So this is a library for HuggingFace datasets. And then we would write this. So we do from datasets would be load dataset. Here we need the name of our dataset. So let's go back to the dataset page. Okay, we can find that at the top here. So it's James Callum, line chain docs. We can just copy it, add that into here. Our split is the training split so that's where the train.json comes in and then we can view the data details there. Okay and once that has loaded we will be able to see we can just kind of extract things so data zero we can see that we have our text in there. So it's super easy to work with and That's kind of like why I recommend storing your data on Hugamvita's datasets If you're wanting to share it and even if you you want them to do the private approach you can you can do that as well You just need I think it's like an API key and that's pretty much it. So that's it for this video I just wanted to cover some of the approaches that we take when we are considering how to chunk our text and actually process it for large language models and also see how we might sort that data later on as well which you know both of these items I think we kind of miss a lot in the typical videos we're really focusing on the large language model processing or the retrieval augmentation or whatever else, right? So this in reality is probably one of the most important parts of the entire process but we miss it pretty often. Anyway, that's it for this video. So thank you very much for watching I hope this is all being useful and interesting and I will see you again in the next one. Bye.\", \"chunks\": [{\"timestamp\": [0.0, 5.56], \"text\": \" In this video, we are going to take a look at what we need to do and what we need to consider\"}, {\"timestamp\": [5.56, 14.62], \"text\": \" when we are chunking text for large language models. The best way I can think of demonstrating\"}, {\"timestamp\": [14.62, 20.26], \"text\": \" this is to walk through an example. Now, we're going to really go with what I believe is kind\"}, {\"timestamp\": [20.26, 25.1], \"text\": \" of like a rule of thumb that I tend to use when I'm chunking text in order to put into a\"}, {\"timestamp\": [25.1, 30.22], \"text\": \" large language model. And it doesn't necessarily apply to every use case. Every use case is\"}, {\"timestamp\": [30.22, 35.86], \"text\": \" slightly different. But I think this is a pretty good approach, at least when we're using retrieval\"}, {\"timestamp\": [35.86, 42.04], \"text\": \" augmentation and large language models, which I think is where the chunking question kind of comes\"}, {\"timestamp\": [42.04, 45.82], \"text\": \" up most often. So let's jump straight into it. In\"}, {\"timestamp\": [45.82, 50.46], \"text\": \" this example what we're going to be doing is taking the langchain docs here\"}, {\"timestamp\": [50.46, 56.76], \"text\": \" literally every page on this website and we're going to be downloading those\"}, {\"timestamp\": [56.76, 62.4], \"text\": \" taking each one of these pages and I'm going to be splitting them into more\"}, {\"timestamp\": [62.4, 66.8], \"text\": \" reasonably sized chunks. Now how are we going to do this?\"}, {\"timestamp\": [66.8, 69.8], \"text\": \" We're going to take a look at this notebook here.\"}, {\"timestamp\": [69.8, 72.1], \"text\": \" Now, if you'd like to follow along with the code,\"}, {\"timestamp\": [72.1, 74.92], \"text\": \" you can also run this notebook.\"}, {\"timestamp\": [74.92, 76.14], \"text\": \" I will leave a link to it,\"}, {\"timestamp\": [76.14, 77.82], \"text\": \" which will appear somewhere near the top\"}, {\"timestamp\": [77.82, 79.2], \"text\": \" of the video right now.\"}, {\"timestamp\": [79.2, 80.22], \"text\": \" Now, to get started,\"}, {\"timestamp\": [80.22, 82.88], \"text\": \" we're going to be using a few Python libraries.\"}, {\"timestamp\": [82.88, 85.94], \"text\": \" Langchain is a pretty big one here.\"}, {\"timestamp\": [85.94, 89.38], \"text\": \" So not only is it the documentation that we're downloading,\"}, {\"timestamp\": [89.38, 92.18], \"text\": \" but it's also going to be how we download\"}, {\"timestamp\": [92.18, 94.98], \"text\": \" that documentation, and it's also going to be\"}, {\"timestamp\": [94.98, 97.62], \"text\": \" how we split that documentation into chunks.\"}, {\"timestamp\": [97.62, 100.76], \"text\": \" And another dependency here is the tick token,\"}, {\"timestamp\": [100.76, 102.86], \"text\": \" tokenizer, we'll talk about that later.\"}, {\"timestamp\": [102.86, 108.26], \"text\": \" And we're just gonna visualize and make things a little bit easier to follow with these libraries here. So in\"}, {\"timestamp\": [108.26, 113.8], \"text\": \" this example first thing I'm going to do is download all of the docs from\"}, {\"timestamp\": [113.8, 120.14], \"text\": \" LineChain. So everything is contained within this is the top level page of the\"}, {\"timestamp\": [120.14, 125.76], \"text\": \" LineChain docs. We're going to save everything into this directory here and\"}, {\"timestamp\": [125.76, 134.52], \"text\": \" we are going to say we want to get all of the dot HTML files. Okay so we run\"}, {\"timestamp\": [134.52, 138.6], \"text\": \" that and that will take a moment just download everything that there's a lot\"}, {\"timestamp\": [138.6, 143.94], \"text\": \" in there. My internet connection is also pretty slow so it will probably take me\"}, {\"timestamp\": [143.94, 145.0], \"text\": \" a moment.\"}, {\"timestamp\": [145.0, 149.4], \"text\": \" But let's go ahead and just have a look at where these are being downloaded.\"}, {\"timestamp\": [149.4, 156.3], \"text\": \" So if we come over to the left here, we can see there is the RTDocs repository there.\"}, {\"timestamp\": [156.3, 161.76], \"text\": \" And inside the RTDocs, we have this line chain read docs, enlatus, which is just kind of\"}, {\"timestamp\": [161.76, 167.88], \"text\": \" like the path of our docs. And okay, cool. So in there,\"}, {\"timestamp\": [167.96, 173.12], \"text\": \" you can see everything's been downloaded. We have like the index page, which I think is the top\"}, {\"timestamp\": [173.12, 179.36], \"text\": \" level page. And you can see it's just HTML. Okay. So it's kind of like, we're not going to process\"}, {\"timestamp\": [179.36, 187.64], \"text\": \" this. We're going to use long chain to clean this up. But if we come down a little bit I think maybe we\"}, {\"timestamp\": [187.64, 192.18], \"text\": \" can see something okay so this is like the the first page welcome to Langchain\"}, {\"timestamp\": [192.18, 198.6], \"text\": \" LLMs are emerging as a transformative technology so on and so on okay and we\"}, {\"timestamp\": [198.6, 206.84], \"text\": \" have some other things of the pages yeah we're just going to process all of this. So back to our code, it's done downloading\"}, {\"timestamp\": [206.84, 212.76], \"text\": \" now. We can come down to here and what we're going to do is use the LankChain document\"}, {\"timestamp\": [212.76, 218.88], \"text\": \" loaders and we're going to use a ReadDops loader. So ReadDops is a specific template\"}, {\"timestamp\": [218.88, 227.52], \"text\": \" that is used quite often for documentation for code libraries. And LimeChain includes a document loader\"}, {\"timestamp\": [227.52, 229.66], \"text\": \" that is specifically built for reading\"}, {\"timestamp\": [229.66, 233.38], \"text\": \" that type of documentation or those HTML pages\"}, {\"timestamp\": [233.38, 236.3], \"text\": \" and processing them into a nicer format.\"}, {\"timestamp\": [236.3, 238.98], \"text\": \" So it's really easy to use it.\"}, {\"timestamp\": [238.98, 243.04], \"text\": \" We just point it to our directory that we just created.\"}, {\"timestamp\": [243.04, 246.3], \"text\": \" And what are we doing here so we're loading\"}, {\"timestamp\": [246.3, 251.5], \"text\": \" those docs and here I'm just printing out the length of those docs so that we\"}, {\"timestamp\": [251.5, 257.96], \"text\": \" can see okay we have 390 HTML pages that have been downloaded there some reason\"}, {\"timestamp\": [257.96, 269.12], \"text\": \" okay so when I when I ran this about an hour ago they they actually had 389 now they have 390 pages so it's already out of date\"}, {\"timestamp\": [269.12, 276.62], \"text\": \" cool all right let's have a look at one of those pages so we have this we have this document object\"}, {\"timestamp\": [276.62, 289.3], \"text\": \" inside that we have page content which is all about text all right if we want to print that in a nicer format, we can see this. Looks pretty\"}, {\"timestamp\": [289.3, 297.84], \"text\": \" good. There is some kind of messy parts of this, but it's not really a problem. We could\"}, {\"timestamp\": [297.84, 305.7], \"text\": \" try and process that if we wanted to, but honestly, I don't really think it's worth it because the large range model can handle this very easily.\"}, {\"timestamp\": [306.88, 311.08], \"text\": \" So, yeah, I personally wouldn't really bother with that.\"}, {\"timestamp\": [311.18, 312.28], \"text\": \" I just take it as it is.\"}, {\"timestamp\": [312.74, 323.06], \"text\": \" Now, at the end of this object, we come right to the end, if it lets me, we'll see that we have this metadata here.\"}, {\"timestamp\": [323.44, 325.9], \"text\": \" we'll see that we have this metadata here. Inside the metadata, we have the source,\"}, {\"timestamp\": [325.9, 330.42], \"text\": \" which is in this case like the file path.\"}, {\"timestamp\": [330.42, 334.14], \"text\": \" But fortunately, the way that we've set this up is that we can\"}, {\"timestamp\": [334.14, 337.3], \"text\": \" just replace RTDocs with HTTPS,\"}, {\"timestamp\": [337.3, 341.1], \"text\": \" and that will give us the URL for this particular file.\"}, {\"timestamp\": [341.1, 344.74], \"text\": \" Let's come down here and you can see that's what I'm doing here,\"}, {\"timestamp\": [344.74, 346.16], \"text\": \" replace RTDocs with\"}, {\"timestamp\": [346.16, 356.34], \"text\": \" HTTPS. Cool. And then we can click that and we come over to here. Now, this is where we start\"}, {\"timestamp\": [356.34, 365.28], \"text\": \" talking about the chunking of what we're doing. When we are thinking about chunking, there are a few things to consider.\"}, {\"timestamp\": [366.28, 375.28], \"text\": \" So the first thing to consider is how much text or how many tokens can our large language model\"}, {\"timestamp\": [375.28, 379.22], \"text\": \" or whatever process is we're doing, how many tokens can it handle?\"}, {\"timestamp\": [379.68, 381.98], \"text\": \" What is optimal for our particular use case?\"}, {\"timestamp\": [387.34, 393.76], \"text\": \" is optimal for our particular use case. The use case that I'm envisioning here is retrieval augmentation for question answering using a larger language model. So what does that mean exactly?\"}, {\"timestamp\": [394.66, 399.88], \"text\": \" It's probably best if I draw it out. So we're going to have our large language model over here\"}, {\"timestamp\": [399.88, 405.5], \"text\": \" and we're going to ask it a question. So we have a question over here supposed to be\"}, {\"timestamp\": [405.5, 411.82], \"text\": \" a queue it's fine so we have our question like we're gonna say what is\"}, {\"timestamp\": [411.82, 417.1], \"text\": \" the LM chain in Langchain right if we pass that straight into our large\"}, {\"timestamp\": [417.1, 424.3], \"text\": \" language model at the moment using a GPT 3.5 turbo even GPT 4 they can't answer\"}, {\"timestamp\": [424.3, 427.44], \"text\": \" that question because they don't know what the langchain library is.\"}, {\"timestamp\": [427.94, 430.52], \"text\": \" So in this scenario,\"}, {\"timestamp\": [430.74, 434.68], \"text\": \" what we would do is we'd go to a vector database.\"}, {\"timestamp\": [435.38, 437.72], \"text\": \" You know, we don't really need to go into too much detail here.\"}, {\"timestamp\": [438.34, 439.52], \"text\": \" We go to a vector database,\"}, {\"timestamp\": [439.84, 442.38], \"text\": \" which is where we saw all of the documents\"}, {\"timestamp\": [442.38, 444.16], \"text\": \" that we're processing now.\"}, {\"timestamp\": [444.16, 449.9], \"text\": \" So all those langchain docs, they would end up within that space and they would be retrieved\"}, {\"timestamp\": [449.9, 456.26], \"text\": \" and we would pass in like five or so of these chunks of text that are relevant\"}, {\"timestamp\": [456.26, 463.28], \"text\": \" to our particular query alongside our original query. So what you'd end up\"}, {\"timestamp\": [463.28, 468.08], \"text\": \" with is rather than, let's say this is your prompt, you typically have\"}, {\"timestamp\": [468.08, 475.92], \"text\": \" your query, rather than just a query, you'd have your query and then you'd also have these five\"}, {\"timestamp\": [476.88, 482.72], \"text\": \" bits of relevant information below the query. And that would all go into the large language model.\"}, {\"timestamp\": [482.72, 485.38], \"text\": \" And you would essentially say to it you'd probably\"}, {\"timestamp\": [485.38, 490.62], \"text\": \" have some instructions near the top and those instructions would say I want you to answer\"}, {\"timestamp\": [490.62, 496.64], \"text\": \" this question you'd maybe give the the questionnaire to give it a bit later on\"}, {\"timestamp\": [496.64, 502.74], \"text\": \" using the context that we have provided and you would basically in front of these contexts you\"}, {\"timestamp\": [502.74, 505.04], \"text\": \" would write like context, okay?\"}, {\"timestamp\": [505.48, 508.26], \"text\": \" And the large language model will answer the question\"}, {\"timestamp\": [508.26, 510.54], \"text\": \" based on those contexts, right?\"}, {\"timestamp\": [510.6, 514.12], \"text\": \" So that's the scenario we're envisioning here.\"}, {\"timestamp\": [514.12, 515.62], \"text\": \" And in this scenario,\"}, {\"timestamp\": [515.84, 520.12], \"text\": \" if we want to input five of these contexts\"}, {\"timestamp\": [520.68, 524.44], \"text\": \" into each one of our retrieval augmented queries,\"}, {\"timestamp\": [528.0, 534.08], \"text\": \" we need to think okay what is the max token limit of our large language model and how much of that space can be reserved for these\"}, {\"timestamp\": [534.08, 551.2], \"text\": \" contexts so in this scenario let's say that we're using gpt 3.5 turbo the token limit for gpt 3.5 Turbo. The token limit for GPT 3.5 Turbo is something like 4096.\"}, {\"timestamp\": [552.88, 559.04], \"text\": \" So this includes both, all right, so you have your large language model. I'm going to put that here.\"}, {\"timestamp\": [559.04, 569.08], \"text\": \" This is, pretend this is your large language model. This 4096 includes the input to the large language model, so all\"}, {\"timestamp\": [569.08, 579.22], \"text\": \" your input tokens, and also all of your generated output tokens. Okay. And so basically, we can't\"}, {\"timestamp\": [579.22, 588.24], \"text\": \" just use that full 4000 tokens on the input, We need to leave some space for the output. And also within the input, we have other components, right?\"}, {\"timestamp\": [588.24, 590.52], \"text\": \" So it's not just the context,\"}, {\"timestamp\": [592.54, 594.46], \"text\": \" but we also have the query.\"}, {\"timestamp\": [596.98, 599.14], \"text\": \" I mean, that's supposed to say query.\"}, {\"timestamp\": [599.14, 602.9], \"text\": \" And as well as that, we might also have some instructions.\"}, {\"timestamp\": [610.54, 615.58], \"text\": \" Don't know why am I writing so bad. And as well as the instructions, we might also have a bit of tracked history if this is a chatbot. Okay.\"}, {\"timestamp\": [616.32, 621.04], \"text\": \" So basically, the amount of context that we can feed in is actually pretty limited.\"}, {\"timestamp\": [621.66, 625.28], \"text\": \" In this scenario, let's just assume that we can we can\"}, {\"timestamp\": [625.28, 631.94], \"text\": \" pass in a context of around half of the 4,000 tokens so we'll say 2,000 is going\"}, {\"timestamp\": [631.94, 640.82], \"text\": \" to be our limit. Okay 2,000 is our limit. That means we need to divide that by\"}, {\"timestamp\": [640.82, 654.46], \"text\": \" 5 because those 2,000 tokens need to be shared by our five contacts which leaves us with about 400 of these tokens per context\"}, {\"timestamp\": [654.46, 659.62], \"text\": \" okay so that's our maximum chunk size now one question that we might have here\"}, {\"timestamp\": [659.62, 667.2], \"text\": \" is could we reduce the number of tokens further and for sure we can okay so i would say the minimum\"}, {\"timestamp\": [667.2, 675.04], \"text\": \" number of tokens that you need within a context is for you to read this context does it make sense\"}, {\"timestamp\": [675.04, 681.74], \"text\": \" right if you have enough words in there for that context to make sense to you as a as a human being\"}, {\"timestamp\": [681.74, 687.76], \"text\": \" then that means that it is probably enough to feed as a chunk of text\"}, {\"timestamp\": [687.76, 695.94], \"text\": \" into a large language model into a embedding model and so on so if that chunk of text has\"}, {\"timestamp\": [695.94, 702.42], \"text\": \" enough text in there to have some sort of meaning to itself then the chunk is probably big enough\"}, {\"timestamp\": [702.42, 710.34], \"text\": \" so as long as you satisfy that that should be the criteria for your minimum size of that chunk of text.\"}, {\"timestamp\": [710.92, 713.88], \"text\": \" Naturally, for the maximum size of a chunk of text,\"}, {\"timestamp\": [713.94, 716.7], \"text\": \" we have the 400 tokens that we just calculated now.\"}, {\"timestamp\": [717.38, 719.5], \"text\": \" So with all that in mind,\"}, {\"timestamp\": [719.58, 725.6], \"text\": \" we need to take a look at how we would actually calculate the size of these chunks.\"}, {\"timestamp\": [725.6, 730.68], \"text\": \" Because we're not basing this on character length, we're basing this on token length.\"}, {\"timestamp\": [730.68, 737.22], \"text\": \" So in order to do that we need to look at how to tokenize text using the same\"}, {\"timestamp\": [737.22, 743.5], \"text\": \" tokenizer that our large language model uses and then we can actually count the\"}, {\"timestamp\": [743.5, 747.92], \"text\": \" number of tokens within each chunk so getting started with\"}, {\"timestamp\": [747.92, 754.24], \"text\": \" that we are going to be using the tick token tokenizer now this is specific to openai models\"}, {\"timestamp\": [754.24, 759.02], \"text\": \" obviously if you're using cohere hugging face and so on this is going to be a slightly different\"}, {\"timestamp\": [759.02, 767.84], \"text\": \" approach so first we want to get our encoding. So there are multiple TikTok and tokenizers that open our users.\"}, {\"timestamp\": [768.08, 769.46], \"text\": \" This is just one of those.\"}, {\"timestamp\": [769.84, 771.6], \"text\": \" Now let's initialize that.\"}, {\"timestamp\": [771.86, 776.08], \"text\": \" And I will talk about a little bit about where we're getting these encoders from.\"}, {\"timestamp\": [776.08, 780.84], \"text\": \" So you can actually find details for the tokenizer at this link here.\"}, {\"timestamp\": [780.94, 788.24], \"text\": \" So this link is in the GitHub repo, TikTok tick token tick token model.py okay so i'm\"}, {\"timestamp\": [788.24, 794.08], \"text\": \" going to click through to that okay so this is in the openai tick token repository on github\"}, {\"timestamp\": [794.08, 800.08], \"text\": \" and you can see we have this model to encoding dictionary here and within this you can see that\"}, {\"timestamp\": [800.08, 806.04], \"text\": \" we have a mapping from each of the models to the particular tokenizer that it uses.\"}, {\"timestamp\": [806.04, 809.66], \"text\": \" We are going to use the GPT 3.5 Turbo model,\"}, {\"timestamp\": [809.66, 813.06], \"text\": \" which uses the CL 100k base.\"}, {\"timestamp\": [813.06, 816.72], \"text\": \" And I would say, I think most of the more recent models,\"}, {\"timestamp\": [816.72, 819.04], \"text\": \" like the models that you'd be using\"}, {\"timestamp\": [819.04, 821.38], \"text\": \" at the time of recording this video,\"}, {\"timestamp\": [821.38, 824.6], \"text\": \" they all use this encoder.\"}, {\"timestamp\": [824.6, 828.26], \"text\": \" Okay, so the embeddings model that is the most up-to-date\"}, {\"timestamp\": [828.26, 838.32], \"text\": \" uses cl 100k base the you know chat gpt's gpt 3.5 turbo uses cl 100k base gpt4 also uses it\"}, {\"timestamp\": [838.32, 845.0], \"text\": \" the only one that is still kind of a relevant model is the text of energy 003 model.\"}, {\"timestamp\": [845.72, 848.14], \"text\": \" And that is the only relevant model\"}, {\"timestamp\": [848.14, 850.74], \"text\": \" that doesn't use that encoder.\"}, {\"timestamp\": [850.74, 853.0], \"text\": \" So this one uses P50K base.\"}, {\"timestamp\": [853.0, 856.5], \"text\": \" All right, so in reality, you don't even need to go there\"}, {\"timestamp\": [856.5, 859.04], \"text\": \" to find out the encoding that you need to use.\"}, {\"timestamp\": [859.04, 859.92], \"text\": \" You can actually just see this.\"}, {\"timestamp\": [859.92, 862.04], \"text\": \" So take token encoding for model\"}, {\"timestamp\": [862.04, 864.72], \"text\": \" and you can run this, right?\"}, {\"timestamp\": [864.72, 867.46], \"text\": \" So you get the CL 100K base.\"}, {\"timestamp\": [867.46, 868.3], \"text\": \" That's how we know.\"}, {\"timestamp\": [869.24, 871.7], \"text\": \" Now, anything else?\"}, {\"timestamp\": [871.7, 873.96], \"text\": \" I think that is pretty much it.\"}, {\"timestamp\": [873.96, 875.24], \"text\": \" So, okay.\"}, {\"timestamp\": [875.24, 876.88], \"text\": \" So actually here,\"}, {\"timestamp\": [876.88, 879.44], \"text\": \" I'm creating this TickToken length function.\"}, {\"timestamp\": [879.44, 882.0], \"text\": \" So that is gonna take some text.\"}, {\"timestamp\": [882.0, 883.78], \"text\": \" It's going to use the tokenizer\"}, {\"timestamp\": [883.78, 886.64], \"text\": \" to calculate the length of that text\"}, {\"timestamp\": [886.64, 893.6], \"text\": \" in terms of tick token tokens that's important because we we need to use that for our line chain\"}, {\"timestamp\": [893.6, 902.72], \"text\": \" splitter function in a moment so we create that then what we can do is just first before we kind\"}, {\"timestamp\": [902.72, 906.0], \"text\": \" of jump into the whole chunking component,\"}, {\"timestamp\": [906.0, 911.52], \"text\": \" I want to have a look at what the length of documents looks like at the moment.\"}, {\"timestamp\": [911.52, 917.12], \"text\": \" So I'm going to calculate the token counts, the tick token length function.\"}, {\"timestamp\": [917.12, 920.72], \"text\": \" Come to here, we can see the minimum, maximum and average number of tokens.\"}, {\"timestamp\": [920.72, 924.48], \"text\": \" So the smallest document contains just 45 tokens.\"}, {\"timestamp\": [924.48, 925.44], \"text\": \" This is probably\"}, {\"timestamp\": [926.32, 932.4], \"text\": \" i don't know this is probably a page that we don't really need it probably doesn't contain anything\"}, {\"timestamp\": [932.4, 940.0], \"text\": \" useful in that maximum is almost 58 000 tokens which is really big i'm not sure i'm not sure\"}, {\"timestamp\": [940.0, 947.68], \"text\": \" what that is but the average is a bit more normal so 1.3 thousand there so we can kind of visualize\"}, {\"timestamp\": [947.68, 955.12], \"text\": \" the distribution of those of those pages and the amount of tokens they have so the vast majority\"}, {\"timestamp\": [955.12, 962.8], \"text\": \" of pages have a very like they're more towards the 1000 token range as we can sort of see here\"}, {\"timestamp\": [962.8, 968.46], \"text\": \" all right cool now let's continue and we'll start\"}, {\"timestamp\": [968.46, 970.74], \"text\": \" and look at how we're going to chunk everything.\"}, {\"timestamp\": [970.74, 972.8], \"text\": \" So again, we're using line chain here.\"}, {\"timestamp\": [972.8, 974.26], \"text\": \" We're using a text splitter\"}, {\"timestamp\": [974.26, 976.5], \"text\": \" and we're using the recursive character text splitter.\"}, {\"timestamp\": [976.5, 980.12], \"text\": \" Now this is, I think probably one of the best\"}, {\"timestamp\": [980.12, 982.9], \"text\": \" like chunkers or text splitters\"}, {\"timestamp\": [982.9, 984.26], \"text\": \" that line chain offers at the moment.\"}, {\"timestamp\": [984.26, 985.5], \"text\": \" It's very general purpose. They do also offer or text splitters that line chain offers at the moment it's very general purpose\"}, {\"timestamp\": [985.5, 993.62], \"text\": \" they do also offer some text splitters that are more specific to like markdown for example but\"}, {\"timestamp\": [993.62, 1000.1], \"text\": \" i you know i i like this one it you can use it for a ton of things so let me just explain it very\"}, {\"timestamp\": [1000.1, 1005.1], \"text\": \" quickly so basically what it's going to do is it's going to take your length\"}, {\"timestamp\": [1005.1, 1009.6], \"text\": \" function so the tick token length and it's going to say I need to split your\"}, {\"timestamp\": [1009.6, 1016.14], \"text\": \" text so that each chunk does not go over this chunk size here so this 400 and\"}, {\"timestamp\": [1016.14, 1021.84], \"text\": \" it's going to split based on these separators. Okay so the reason we have\"}, {\"timestamp\": [1021.84, 1026.38], \"text\": \" multiple separators is that it first starts by trying to find double new lines\"}, {\"timestamp\": [1026.38, 1028.38], \"text\": \" So this is a double new line separator\"}, {\"timestamp\": [1028.58, 1034.92], \"text\": \" It's going to try and split on that first if it can't find a good split using the double new line\"}, {\"timestamp\": [1035.9, 1038.56], \"text\": \" Characters it will just try a single new line\"}, {\"timestamp\": [1038.78, 1043.24], \"text\": \" Then it will try a space and as a very last resort it will just split on anything\"}, {\"timestamp\": [1043.48, 1045.4], \"text\": \" Okay, okay cool and\"}, {\"timestamp\": [1045.4, 1050.38], \"text\": \" then one final thing that we have here is this chunk overlap so this chunk\"}, {\"timestamp\": [1050.38, 1056.26], \"text\": \" overlap is saying for every chunk we are going to overlap it with the next chunk\"}, {\"timestamp\": [1056.26, 1068.62], \"text\": \" by 20 tokens okay let me let me draw that out so it makes more sense. Okay, so imagine we have a ton of texts, okay?\"}, {\"timestamp\": [1068.62, 1069.92], \"text\": \" There's loads of texts here.\"}, {\"timestamp\": [1072.16, 1077.12], \"text\": \" Okay, now we are going to get a chunk of,\"}, {\"timestamp\": [1077.12, 1078.38], \"text\": \" it's 400 characters, right?\"}, {\"timestamp\": [1078.38, 1082.38], \"text\": \" So let's say that chunk takes us from here\"}, {\"timestamp\": [1082.38, 1084.96], \"text\": \" all the way to, let's say here.\"}, {\"timestamp\": [1084.96, 1087.56], \"text\": \" Okay, so we have 400 characters in\"}, {\"timestamp\": [1087.56, 1093.64], \"text\": \" this chunk. Then the next chunk if we don't have any chunk overlap would be\"}, {\"timestamp\": [1093.64, 1100.64], \"text\": \" 400 characters from this so that would be let's say it's to here. Okay but this\"}, {\"timestamp\": [1100.64, 1108.26], \"text\": \" comes with a problem because we don't know what this information here and this information here is about.\"}, {\"timestamp\": [1108.52, 1111.04], \"text\": \" So they could be related, right?\"}, {\"timestamp\": [1111.1, 1116.34], \"text\": \" So we might be missing out on some important information by just splitting in the middle here.\"}, {\"timestamp\": [1116.88, 1121.0], \"text\": \" So it's important to try and avoid that if possible.\"}, {\"timestamp\": [1121.18, 1126.24], \"text\": \" And the most naive way or naive approach for doing this is to\"}, {\"timestamp\": [1126.24, 1135.04], \"text\": \" include a trunk overlap so what we would do is let's say we take the 20 tokens\"}, {\"timestamp\": [1135.04, 1148.0], \"text\": \" behind this okay so we're gonna go back 20 tokens which maybe comes to here okay so that means that this space here is now going to be\"}, {\"timestamp\": [1148.0, 1156.88], \"text\": \" shared by the last or the the first chunk and the next chunk which will also bring back the\"}, {\"timestamp\": [1156.88, 1168.24], \"text\": \" next chunk to something like here right so now we have chunk one here, which goes from here up to here.\"}, {\"timestamp\": [1168.96, 1175.94], \"text\": \" And then we have chunk two, which is from here to here.\"}, {\"timestamp\": [1176.94, 1183.6], \"text\": \" Then following on from that, we would also add another chunk overlap for number three.\"}, {\"timestamp\": [1183.74, 1187.82], \"text\": \" So number three would go from here to let's say here.\"}, {\"timestamp\": [1187.82, 1189.32], \"text\": \" And finally for number four,\"}, {\"timestamp\": [1189.32, 1191.36], \"text\": \" we go from like here to here.\"}, {\"timestamp\": [1191.36, 1194.06], \"text\": \" Okay, so the chunk overlap is just to make sure\"}, {\"timestamp\": [1194.06, 1196.98], \"text\": \" that we're not missing any important connections\"}, {\"timestamp\": [1196.98, 1198.74], \"text\": \" between our chunks.\"}, {\"timestamp\": [1198.74, 1201.3], \"text\": \" Okay, it does mean that we're going to have\"}, {\"timestamp\": [1201.3, 1208.56], \"text\": \" a little bit more data to solve there. there okay because we're including like these chunks of\"}, {\"timestamp\": [1208.56, 1217.26], \"text\": \" 20 in multiple places but I think that's usually worth it in terms of the better performance that\"}, {\"timestamp\": [1217.26, 1222.32], \"text\": \" you can get by not missing out that important information that important connection between\"}, {\"timestamp\": [1222.32, 1225.5], \"text\": \" chunks okay so we initialize\"}, {\"timestamp\": [1225.5, 1231.1], \"text\": \" that and then to actually split the text we use the text splitter split text okay\"}, {\"timestamp\": [1231.1, 1235.88], \"text\": \" we're going to take DOPS 5 and we're going to take the page content okay which is\"}, {\"timestamp\": [1235.88, 1243.22], \"text\": \" just the plain text. Right so based on the parameters that we set here\"}, {\"timestamp\": [1243.22, 1246.04], \"text\": \" chunk size of 400 and chunk overlap of 20\"}, {\"timestamp\": [1246.04, 1250.06], \"text\": \" using the tick token length token, we get two chunks.\"}, {\"timestamp\": [1250.56, 1252.42], \"text\": \" Let's have a look at the length of those two chunks.\"}, {\"timestamp\": [1253.58, 1257.86], \"text\": \" Okay, so the first chunk that we get is 346 tokens.\"}, {\"timestamp\": [1258.08, 1259.74], \"text\": \" Next one, 247.\"}, {\"timestamp\": [1259.86, 1264.5], \"text\": \" So both within that max upper end limit of 400.\"}, {\"timestamp\": [1265.0, 1265.2], \"text\": \" Okay, so you see that it's not going to necessarily split both within that max upper end limit of 400.\"}, {\"timestamp\": [1268.98, 1271.58], \"text\": \" Okay, so you see that it's not going to necessarily split on the 400 tokens specifically\"}, {\"timestamp\": [1271.58, 1274.88], \"text\": \" because we have the specific separators\"}, {\"timestamp\": [1274.88, 1276.76], \"text\": \" that we would like to use, okay?\"}, {\"timestamp\": [1276.76, 1281.46], \"text\": \" And it's going to optimize preferably for this separator.\"}, {\"timestamp\": [1281.46, 1284.06], \"text\": \" Okay, so we're not going right up to that limit\"}, {\"timestamp\": [1284.06, 1287.02], \"text\": \" with every single chunk, which is fine.\"}, {\"timestamp\": [1287.02, 1288.24], \"text\": \" That's kind of ideal.\"}, {\"timestamp\": [1288.24, 1292.66], \"text\": \" We don't necessarily need to put in a ton of text there.\"}, {\"timestamp\": [1294.62, 1297.86], \"text\": \" So that's it for a single document.\"}, {\"timestamp\": [1297.86, 1300.64], \"text\": \" What we're going to do now is we're going to repeat that\"}, {\"timestamp\": [1300.64, 1302.84], \"text\": \" over the entire dataset.\"}, {\"timestamp\": [1302.84, 1306.08], \"text\": \" And the final format that I want to create here is going to look like\"}, {\"timestamp\": [1306.08, 1309.94], \"text\": \" this okay so we're going to have the id we're going to have our text I'm going to have the source\"}, {\"timestamp\": [1309.94, 1319.22], \"text\": \" where this text has actually come from okay now one thing that you'll notice here is the id okay\"}, {\"timestamp\": [1319.22, 1326.54], \"text\": \" so we're going to create an id and that id will be unique to each page okay but we're\"}, {\"timestamp\": [1326.54, 1330.62], \"text\": \" going to have multiple chunks for each page so that means we're also going to\"}, {\"timestamp\": [1330.62, 1334.82], \"text\": \" add in this like chunk identifier onto the end of the ID to make sure that\"}, {\"timestamp\": [1334.82, 1341.48], \"text\": \" every ID for every chunk is actually unique so let me show you how we're\"}, {\"timestamp\": [1341.48, 1345.84], \"text\": \" going to create that essentially so we have the URL here how we're going to create that. Essentially, so we have the URL here.\"}, {\"timestamp\": [1345.84, 1350.66], \"text\": \" We're going to replace the RT docs that we have here\"}, {\"timestamp\": [1350.66, 1354.64], \"text\": \" with the actual HTTPS protocol.\"}, {\"timestamp\": [1354.64, 1357.98], \"text\": \" And I'm just going to print out so you can see what it is.\"}, {\"timestamp\": [1357.98, 1360.4], \"text\": \" And then we're going to take that URL.\"}, {\"timestamp\": [1360.4, 1363.16], \"text\": \" We're going to add it to this hashlib MD5.\"}, {\"timestamp\": [1363.16, 1371.38], \"text\": \" So this is just a hashing function that is going to take our URL and hash it into kind of like a unique identifier.\"}, {\"timestamp\": [1372.06, 1372.24], \"text\": \" Right.\"}, {\"timestamp\": [1372.88, 1380.32], \"text\": \" So this is useful because if we are updating this text at some point in the future or this data set, sorry,\"}, {\"timestamp\": [1380.78, 1385.34], \"text\": \" we can use the same hashing function to create our unique ids and that means that when\"}, {\"timestamp\": [1385.34, 1393.98], \"text\": \" we update this particular page it will just overwrite the previous versions of that item\"}, {\"timestamp\": [1393.98, 1398.66], \"text\": \" all right because we're using the same id but of course we don't we can't use the same id for every\"}, {\"timestamp\": [1398.66, 1407.36], \"text\": \" single chunk so we also need to add in this here which is like the the chunk identifier right it's just it's just a\"}, {\"timestamp\": [1407.36, 1413.84], \"text\": \" count of the number of chunks so we can see that being created here so these are just two examples\"}, {\"timestamp\": [1413.84, 1420.8], \"text\": \" from the previous page that we we just showed so you can see we have the trunk identifier and\"}, {\"timestamp\": [1420.8, 1427.24], \"text\": \" indeed the chunks are different so this says language model cascades ice primer books socratic\"}, {\"timestamp\": [1427.24, 1434.88], \"text\": \" models okay whatever let's take a look at what is at the end of the first item and it should be\"}, {\"timestamp\": [1434.88, 1445.68], \"text\": \" something similar so there should be the overlap that i mentioned right okay so yeah you can see language model cascades, ISE prime books, Socratic\"}, {\"timestamp\": [1445.68, 1452.66], \"text\": \" models, right same thing, cool. So there is the overlap right now what we need to do\"}, {\"timestamp\": [1452.66, 1456.7], \"text\": \" is repeat this same logic that we've just created across our entire data set.\"}, {\"timestamp\": [1456.7, 1461.0], \"text\": \" So to do that same thing that we just did we're going to take the URL out, we're\"}, {\"timestamp\": [1461.0, 1465.12], \"text\": \" going to create our unique ID, we're going to take chunks using the text splitter,\"}, {\"timestamp\": [1465.12, 1471.6], \"text\": \" and then we're going to append these all to our documents list here. Okay, that's just going to\"}, {\"timestamp\": [1472.24, 1478.16], \"text\": \" be where we saw everything. Okay, and now, so the length of the documents\"}, {\"timestamp\": [1479.12, 1489.54], \"text\": \" an hour ago was a little bit less. Now it 2012 documents so sorry 2,212\"}, {\"timestamp\": [1489.54, 1496.04], \"text\": \" documents cool we can now save them to JSON lines file to that we just do this\"}, {\"timestamp\": [1496.04, 1502.62], \"text\": \" so JSON lines is basically it's what you can see here alright so if we take a\"}, {\"timestamp\": [1502.62, 1506.34], \"text\": \" look at the documents first five it's this but it's just in can see here, right? So if we take a look at the documents, take a look at the first five, it's this,\"}, {\"timestamp\": [1506.34, 1510.28], \"text\": \" but it's just in a JSON lines file, okay?\"}, {\"timestamp\": [1510.28, 1514.62], \"text\": \" So you can see it here, yeah, same thing, right?\"}, {\"timestamp\": [1514.62, 1516.66], \"text\": \" Okay, and then once you've saved it\"}, {\"timestamp\": [1516.66, 1518.12], \"text\": \" and you've created your JSONL file,\"}, {\"timestamp\": [1518.12, 1520.24], \"text\": \" you would just load it from file like this, okay?\"}, {\"timestamp\": [1520.24, 1525.44], \"text\": \" So you, with open, train JSONL, wherever you saw it,\"}, {\"timestamp\": [1525.44, 1528.12], \"text\": \" and you just load it iteratively like that.\"}, {\"timestamp\": [1529.1, 1531.1], \"text\": \" Okay, and you can take a look.\"}, {\"timestamp\": [1532.18, 1533.22], \"text\": \" Yeah, okay, great.\"}, {\"timestamp\": [1533.22, 1535.2], \"text\": \" So that's how you would load it.\"}, {\"timestamp\": [1535.2, 1537.72], \"text\": \" Now, couple of things here.\"}, {\"timestamp\": [1537.72, 1539.66], \"text\": \" The reason that we're using JSONL\"}, {\"timestamp\": [1539.66, 1542.62], \"text\": \" and the reason I'm calling this train.jsonl\"}, {\"timestamp\": [1542.62, 1547.22], \"text\": \" is because this makes it very compatible with HuggingFace datasets\"}, {\"timestamp\": [1547.22, 1554.2], \"text\": \" which is essentially a way of sharing your dataset with others or just making it more\"}, {\"timestamp\": [1554.2, 1559.02], \"text\": \" accessible for yourself if you set to being a private dataset. So what I want to do is just\"}, {\"timestamp\": [1559.02, 1571.56], \"text\": \" show you how we can actually go about doing that as well. So the first thing that we need to do is go to huggingface.co and that will bring you to the first page of Hugging Face which may look different to\"}, {\"timestamp\": [1571.56, 1578.38], \"text\": \" you because you may not already have an account on Hugging Face. So if you do need an account or\"}, {\"timestamp\": [1578.38, 1583.38], \"text\": \" you need to sign in there will be a little button over here that says sign up or log in so you would\"}, {\"timestamp\": [1583.38, 1590.16], \"text\": \" follow that create your account or log in and then you will see something like this at which point you go over to your profile\"}, {\"timestamp\": [1590.16, 1596.88], \"text\": \" you click new data set we give our data set a name i'm going to call it langchain dots you can\"}, {\"timestamp\": [1596.88, 1601.76], \"text\": \" obviously call this whatever you want you can set it to private if you want to keep this data set\"}, {\"timestamp\": [1601.76, 1606.04], \"text\": \" private for me also i'm going to just leave it as public, and you create your dataset.\"}, {\"timestamp\": [1606.76, 1611.1], \"text\": \" So on here, this is like the page of your dataset,\"}, {\"timestamp\": [1611.26, 1612.6], \"text\": \" like the homepage of your dataset.\"}, {\"timestamp\": [1612.96, 1614.3], \"text\": \" You go to Files.\"}, {\"timestamp\": [1614.8, 1617.3], \"text\": \" You go to Add File, Upload Files.\"}, {\"timestamp\": [1618.4, 1625.0], \"text\": \" And then you just need to drag in the train.jsonl file to here.\"}, {\"timestamp\": [1625.82, 1628.1], \"text\": \" So for me, that is here.\"}, {\"timestamp\": [1628.1, 1630.24], \"text\": \" I'm just gonna go and drag that in.\"}, {\"timestamp\": [1630.24, 1633.74], \"text\": \" Okay, we go down, commit changes to main.\"}, {\"timestamp\": [1633.74, 1636.84], \"text\": \" Okay, so we have now uploaded that.\"}, {\"timestamp\": [1636.84, 1639.14], \"text\": \" We can go click on files here and we'll be able to see\"}, {\"timestamp\": [1639.14, 1642.94], \"text\": \" that we have the train.jsonl file in there.\"}, {\"timestamp\": [1642.94, 1646.18], \"text\": \" Now to actually use that in our code, we would need to\"}, {\"timestamp\": [1646.18, 1651.0], \"text\": \" pip install datasets. So this is a library for HuggingFace datasets.\"}, {\"timestamp\": [1651.0, 1665.0], \"text\": \" And then we would write this. So we do from datasets would be load dataset.\"}, {\"timestamp\": [1667.94, 1671.58], \"text\": \" Here we need the name of our dataset.\"}, {\"timestamp\": [1671.58, 1675.34], \"text\": \" So let's go back to the dataset page.\"}, {\"timestamp\": [1675.34, 1676.76], \"text\": \" Okay, we can find that at the top here.\"}, {\"timestamp\": [1676.76, 1678.98], \"text\": \" So it's James Callum, line chain docs.\"}, {\"timestamp\": [1678.98, 1681.68], \"text\": \" We can just copy it, add that into here.\"}, {\"timestamp\": [1682.62, 1689.34], \"text\": \" Our split is the training split so that's where the train.json\"}, {\"timestamp\": [1689.34, 1694.16], \"text\": \" comes in and then we can view the data details there. Okay and once that has\"}, {\"timestamp\": [1694.16, 1700.98], \"text\": \" loaded we will be able to see we can just kind of extract things so data zero\"}, {\"timestamp\": [1700.98, 1706.08], \"text\": \" we can see that we have our text in there. So it's super easy to work with and\"}, {\"timestamp\": [1706.66, 1708.66], \"text\": \" That's kind of like why I recommend\"}, {\"timestamp\": [1709.06, 1711.14], \"text\": \" storing your data on Hugamvita's datasets\"}, {\"timestamp\": [1711.7, 1717.14], \"text\": \" If you're wanting to share it and even if you you want them to do the private approach you can you can do that as well\"}, {\"timestamp\": [1717.18, 1722.1], \"text\": \" You just need I think it's like an API key and that's pretty much it. So that's it for this video\"}, {\"timestamp\": [1722.1, 1727.24], \"text\": \" I just wanted to cover some of the approaches that\"}, {\"timestamp\": [1727.24, 1734.54], \"text\": \" we take when we are considering how to chunk our text and actually process it for large language\"}, {\"timestamp\": [1734.54, 1743.26], \"text\": \" models and also see how we might sort that data later on as well which you know both of these\"}, {\"timestamp\": [1743.26, 1746.32], \"text\": \" items I think we kind of miss a lot in the typical videos\"}, {\"timestamp\": [1746.32, 1753.12], \"text\": \" we're really focusing on the large language model processing or the retrieval augmentation or\"}, {\"timestamp\": [1753.12, 1758.08], \"text\": \" whatever else, right? So this in reality is probably one of the most important parts of\"}, {\"timestamp\": [1758.08, 1764.0], \"text\": \" the entire process but we miss it pretty often. Anyway, that's it for this video.\"}, {\"timestamp\": [1764.72, 1766.3], \"text\": \" So thank you very much\"}, {\"timestamp\": [1766.3, 1771.82], \"text\": \" for watching I hope this is all being useful and interesting and I will see\"}, {\"timestamp\": [1771.82, 1775.42], \"text\": \" you again in the next one. Bye.\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getdefaultlocale()\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "8Y_n6afS16qs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/suno-ai/bark.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx3YYeqNz5TY",
        "outputId": "4855575d-8d3e-40b3-a8c8-7c1d409983aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/suno-ai/bark.git\n",
            "  Cloning https://github.com/suno-ai/bark.git to /tmp/pip-req-build-luse2ihe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/suno-ai/bark.git /tmp/pip-req-build-luse2ihe\n",
            "  Resolved https://github.com/suno-ai/bark.git to commit 773624d26db84278a55aacae9a16d7b25fbccab8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3 (from suno-bark==0.0.1a0)\n",
            "  Downloading boto3-1.33.6-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting encodec (from suno-bark==0.0.1a0)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting funcy (from suno-bark==0.0.1a0)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (0.19.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (1.11.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (0.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (23.2)\n",
            "Collecting botocore<1.34.0,>=1.33.6 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from encodec->suno-bark==0.0.1a0) (2.1.0+cu118)\n",
            "Collecting einops (from encodec->suno-bark==0.0.1a0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->suno-bark==0.0.1a0) (2023.6.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->suno-bark==0.0.1a0) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.0,>=1.33.6->boto3->suno-bark==0.0.1a0) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.0,>=1.33.6->boto3->suno-bark==0.0.1a0) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->suno-bark==0.0.1a0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->suno-bark==0.0.1a0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.6->boto3->suno-bark==0.0.1a0) (1.16.0)\n",
            "Building wheels for collected packages: suno-bark, encodec\n",
            "  Building wheel for suno-bark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for suno-bark: filename=suno_bark-0.0.1a0-py3-none-any.whl size=2567413 sha256=d19d87aec51a51c81bced005d29a8bcbaba618c3a314ce9c38a0df1d85a7eb7a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-10tjead_/wheels/e6/6d/c2/107ed849afe600f905bb4049a026df3c7c5aa75d86c2721ec7\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=ef0d0c1892a7e25794e1f6adb4017c0c2e28dd8699f2a651ff43f0eae8f031cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "Successfully built suno-bark encodec\n",
            "Installing collected packages: funcy, jmespath, einops, botocore, s3transfer, encodec, boto3, suno-bark\n",
            "Successfully installed boto3-1.33.6 botocore-1.33.6 einops-0.7.0 encodec-0.1.1 funcy-2.0 jmespath-1.0.1 s3transfer-0.8.2 suno-bark-0.0.1a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "7a1d88a520984282840d9cd5ad54a9f9",
            "5f53441d17114364a92342a2626bc7ae",
            "d760068a93af4cad943252554b91ab0f",
            "d7ed61bff0284ece919de1b68130509e",
            "3243d73bcf1c46899fb8873a03e25e49",
            "22cee060a8994847aa54902448f04b63",
            "b7d9177c16a342c78867145b98f6282f",
            "943995c5ead5440b8d61cc5d52beb4d2",
            "5f6987d7ae164814aaaa2f3567cba2b1",
            "cdc1c8924a614cf18e6fb3562c3a8678",
            "1bece5c4333a484ea8386f18c102e174",
            "e3024908fb3741199b9861462cd056fa",
            "37d7e4e1771f4008a2299b6dae8e7db7",
            "e1b60cc0fc514016885cb9e694201567",
            "4c32f7499cfb4b0e8cbeaf85c145c52a",
            "55e694c6b9fa4a8487a1e9d0ac4f37ab",
            "abdb36c198334fb0a48f0aa4ae768509",
            "c1cc090e652b4ef09a28bddd33a51672",
            "8e50b5449ed147c899ec46ce3fcb5ec4",
            "f1024d33a46243ff9fd759dbb9083455",
            "316740d1905a4f64b5d7c32fc54e27a8",
            "977c201d434942b48426fc37f7665c8a",
            "619b7988e6f94f6aaee6d4d4ec46e419",
            "eb84e2f9a4a149b29583e12959c0d552",
            "e421d7d69ec948efa671f33d040b5cbf",
            "964e2beafa8d4cbc996d81c0bdcd0548",
            "f66cc1acb384455c9eeda86ba662d945",
            "276ac35ae33e48f48b3f676e5893525a",
            "34ee97b84ef240a982fc1c44f04e6c92",
            "7641dfd3cc76475f9c751fa2cc06bdae",
            "8c1f350274b24156a9d78e61762f3919",
            "718322aea83b4f889c92b6cc81329be4",
            "6ffc6f9b17ca4143a7aa6a672a315177",
            "8def9dd093c74719a820b59dbaf3084a",
            "922cf4ae6ea343e584418c2e4960ec0c",
            "2988d91762154e5a84390be344cd51bc",
            "aaf2ba9341b44cc3bf8df84890d121a4",
            "881a811584964935a1a3c39343b7a64a",
            "27fa0489ac6047889c4ecb6a79812bc4",
            "bbc1475c97234d4d939170b94d0f5d6d",
            "bfbb00fc2d4f409fb3a428dee3cd17e9",
            "c6fdbdc40bea43328ad7d85b868a1e28",
            "442c92a6329944ec905b6a786cfcb65d",
            "ca4308264f1645a18cf1d3cc13318b0c",
            "bed09f6a44494bf9ab640476ddeba455",
            "5e63409b96be46e4878426075a1e3303",
            "f8b2db2a4b134069a88e0333fc870214",
            "ac2d35c4d3794d3faf0a6a53a2e533e5",
            "3ad28a0330fb49639c10be1e93fd737e",
            "4d96ab26cb794cc8bbb502499ae07a3a",
            "cb386ed3259648e6ba3408c16371d464",
            "ab160c335a1a4c41950e948fab4676c4",
            "9a672abcfbeb4374b0d15387740ec1e5",
            "2193b39b8fc746ce9859d0f76ed4583e",
            "f3c5847375174aea868ca7c583c3a127",
            "565afd87d245499fb13d85c24aa8d0c6",
            "f50f35be922e46078667e9f953cc1872",
            "69919f5070db4a91a149cebe03d7615b",
            "ee98e0f2966348f9a203efa4beb83b3f",
            "65cf19c451e74a238ca751f57479ce1e",
            "0082f215390d4a03820ffd860ca589ac",
            "2683aaee6b0d4530bbacfd72de6257ad",
            "801a75ab157b48ac98dc432005ef6747",
            "54aa4da93f2047fca8c4efe50f0772dc",
            "03ef0f08cc5d4897bb9535b288ceb2a3",
            "d4cbaafa985548d3b5b6e35e8c966557",
            "b59050071f744bbb9da3abc35b18883d",
            "74f395f0004f4434b9d077c9344ae491",
            "0c0e84424df441f1a9945cc155f393f1",
            "08220cf957784d4cb91a123a2c06b199",
            "4a23026f58d2419ca8b64558ef528e73",
            "776d592b26744d838879860be29c970b",
            "e8697d8c87dd4011b0a4cf3863e08846",
            "56d1d963cf9e425db7c0241483f9c9d7",
            "685167286add44b2a8b9c3185d75fc7f",
            "f884c595489945a79033d3d6adb94a2b",
            "19a7f478e2b24515a94a6d7011873e16"
          ]
        },
        "id": "i0Ft2co9zcWO",
        "outputId": "3b3a466a-fcd5-4347-a3e8-65113e7c4ab1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a1d88a520984282840d9cd5ad54a9f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3024908fb3741199b9861462cd056fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "619b7988e6f94f6aaee6d4d4ec46e419"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8def9dd093c74719a820b59dbaf3084a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "source.spm:   0%|          | 0.00/806k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bed09f6a44494bf9ab640476ddeba455"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "565afd87d245499fb13d85c24aa8d0c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59050071f744bbb9da3abc35b18883d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "text_to_speech_convertor = pipeline(\"text-to-speech\", model=\"suno/bark\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "xU6afF7T2Uhp",
        "outputId": "04ddef19-13a7-4345-ca3e-b9e81c5274f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f20e6099167>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_to_speech_convertor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-to-speech\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"suno/bark\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in result[\"chunks\"][0:10]:\n",
        "  translate_text = translator(row[\"text\"])\n",
        "  print(\"origin:%s\"%row[\"text\"])\n",
        "  print(\"translate:%s\"%(translate_text))\n",
        "  speech = text_to_speech_convertor(translate_text)\n",
        "  # scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n",
        "  # Audio(data=audio_data, rate=22050)\n",
        "  Audio(rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SRj1BvowO-K",
        "outputId": "2401e516-fc23-49e7-de1d-c65330666801"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin: In this video, we are going to take a look at what we need to do and what we need to consider\n",
            "translate:[{'translation_text': '在这个视频中,我们将审视我们需要做什么和需要考虑什么。'}]\n",
            "origin: when we are chunking text for large language models. The best way I can think of demonstrating\n",
            "translate:[{'translation_text': '当我们为大语言模型的文本加块时。 最好的演示方式'}]\n",
            "origin: this is to walk through an example. Now, we're going to really go with what I believe is kind\n",
            "translate:[{'translation_text': '举个例子。现在,我们要用我认为是善良的'}]\n",
            "origin: of like a rule of thumb that I tend to use when I'm chunking text in order to put into a\n",
            "translate:[{'translation_text': '就像一个大拇指规则,我倾向于使用 当我在填充文字时, 以便放入一个'}]\n",
            "origin: large language model. And it doesn't necessarily apply to every use case. Every use case is\n",
            "translate:[{'translation_text': '大语言模型。 它不一定适用于每个使用案例。 每个使用案例都是'}]\n",
            "origin: slightly different. But I think this is a pretty good approach, at least when we're using retrieval\n",
            "translate:[{'translation_text': '稍稍不同,但我认为这是一个相当不错的方法, 至少当我们使用检索时'}]\n",
            "origin: augmentation and large language models, which I think is where the chunking question kind of comes\n",
            "translate:[{'translation_text': '和大语言模型, 我认为这就是 块状问题 种类的到来'}]\n",
            "origin: up most often. So let's jump straight into it. In\n",
            "translate:[{'translation_text': '我们直接跳进去吧'}]\n",
            "origin: this example what we're going to be doing is taking the langchain docs here\n",
            "translate:[{'translation_text': '举个例子,我们将要做的就是把 长链文件带到这里'}]\n",
            "origin: literally every page on this website and we're going to be downloading those\n",
            "translate:[{'translation_text': '这个网站上的每个页面 字字字字字数的每个页面 我们将要下载这些'}]\n"
          ]
        }
      ]
    }
  ]
}